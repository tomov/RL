compare with

iterative policy evaluation p 99 of sutton & barto
better: policy iteration p 105
evern better: value iteration p 110

async DP -- p 113

on-policy monte carlo p 134
better: incremental monte carlo p 139

FUCK That -- TD p 145
SARSA -- p 156
Q learning p 158

SARSA(lambda) p 184
Q(lambda) p 187


model-based: DP and heuristic search
model-free: monte carlo & TD

Dyna-Q -- p 203
dyna-Q(lambda) ?
stochastic dyna-Q -- belief states

prioritized sweeping

Kalman-TD
implement Kalman, test w/ context task
Kalman-dyna-TD?

heuristic search = forward planning + TD(lambda) p 222

gradient-descent sarsa(lambda)

actor-critic? p 236


two problems: policy evaluation = given policy, evaluate v(s)
policy improvement = given v(s), find policy





create base MDP class for Q, SARSA, and LMDP
create separate environment from agent

kalman-TD?

successor-feature approach (Barreto 2016?)


CODE:

TD(lambda)

make hmlmdp's local and online

options with actor-critic
- include reward R + R~ in calculating option policy?

taxi domain

intra-option learning and whatnot from Sutton:



IDEAS:

SARSA for option discovery (s,a) --> (s', a')
Bayesian RL ? => for credit assignment
salience, novelty

EXPERIMENTS:

"which of these two situations is more valuable?"

PAPERS:

finding the options? -- laplace
Variable-reward hierarchical reinforcement learning (Mehta et al., 2008)   <-------- !!!!
Wilson et al. (2007) consider learning in a hierarchical Bayesian RL <--- !!! bayesian prior distribution over tasks
HAM framework
look into bayesian RL


----------------------------


maybe focus on Options first? play around w/ them?

maybe actually create tasks for yourself and play around w/ them? observe your behavior #introspection

to start with (for andrew's thing):
- everything deterministic

    m
