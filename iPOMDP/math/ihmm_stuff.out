\BOOKMARK [1][-]{section.1}{Background}{}% 1
\BOOKMARK [2][-]{subsection.1.1}{Reinforcement Learning and Markov Decision Processes}{section.1}% 2
\BOOKMARK [3][-]{subsubsection.1.1.1}{RL in the Brain}{subsection.1.1}% 3
\BOOKMARK [2][-]{subsection.1.2}{Hierarchical Reinforcement Learning and Semi-Markov Decision Processes}{section.1}% 4
\BOOKMARK [3][-]{subsubsection.1.2.1}{HRL in the Brain}{subsection.1.2}% 5
\BOOKMARK [2][-]{subsection.1.3}{Option Discovery and Transfer}{section.1}% 6
\BOOKMARK [2][-]{subsection.1.4}{Multi-task Reinforcement Learning}{section.1}% 7
\BOOKMARK [3][-]{subsubsection.1.4.1}{MTRL for HRL}{subsection.1.4}% 8
\BOOKMARK [2][-]{subsection.1.5}{Partially Observable Markov Decision Processes}{section.1}% 9
\BOOKMARK [3][-]{subsubsection.1.5.1}{POMDPs in the Brain}{subsection.1.5}% 10
\BOOKMARK [2][-]{subsection.1.6}{Infinite POMDPs}{section.1}% 11
\BOOKMARK [1][-]{section.2}{The Model}{}% 12
\BOOKMARK [2][-]{subsection.2.1}{Modular iPOMDPs}{section.2}% 13
\BOOKMARK [3][-]{subsubsection.2.1.1}{Modular iPOMDPs using repeated transition functions}{subsection.2.1}% 14
\BOOKMARK [3][-]{subsubsection.2.1.2}{Modular iPOMDPs using nested iPOMDPs}{subsection.2.1}% 15
\BOOKMARK [2][-]{subsection.2.2}{MTRL with modular iPOMDPs}{section.2}% 16
\BOOKMARK [3][-]{subsubsection.2.2.1}{Shared modules}{subsection.2.2}% 17
\BOOKMARK [3][-]{subsubsection.2.2.2}{Shared observations}{subsection.2.2}% 18
\BOOKMARK [3][-]{subsubsection.2.2.3}{Shared rewards}{subsection.2.2}% 19
\BOOKMARK [2][-]{subsection.2.3}{hierarchical iHMM as HDP}{section.2}% 20
\BOOKMARK [2][-]{subsection.2.4}{hierarchical iHMM as HDP, version 2}{section.2}% 21
\BOOKMARK [2][-]{subsection.2.5}{hierarchical iHMM as HDP, version 3}{section.2}% 22
